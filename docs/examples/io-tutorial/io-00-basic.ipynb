{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698e8919-31fb-4da5-8db3-ed0313944502",
   "metadata": {},
   "source": [
    "# dask-awkward IO Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674364a-c9d2-434f-97c7-b27d703f75bf",
   "metadata": {},
   "source": [
    "_last updated 2024-01-26_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f369935-c703-43d8-8c77-64fe94b391a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import awkward\n",
    "import dask_awkward\n",
    "print(\"package versions:\")\n",
    "print(f\"awkward:       {awkward.__version__}\")\n",
    "print(f\"dask-awkward:  {dask_awkward.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f366f-8fad-477e-b872-9e1af0f8012e",
   "metadata": {},
   "source": [
    "## I. The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455b808-73e0-4e3b-ac70-9efbc2c2b555",
   "metadata": {},
   "source": [
    "### Data on disk\n",
    "\n",
    "`dask-awkward` supports a number of file formats out-of-the-box. Those include:\n",
    "\n",
    "- Parquet\n",
    "- JSON\n",
    "- Plain text\n",
    "\n",
    "\n",
    "> _Note_: The [uproot project](https://github.com/scikit-hep/uproot5) provides the `uproot.dask` module for reading the ROOT file format into dask-awkward collections. This tutorial will focus on file formats that have support baked into `dask-awkward`.\n",
    "\n",
    "Since this is Dask, data on disk is _staged_ for reading when using a dask-awkward read function. Data will not be read until a `compute` or (`persist`) call is reached.\n",
    "\n",
    "**Note**: we can also create dask-awkward `Array` collections from other Dask collections (bag, array, dataframe, delayed) or from data that already exists in memory (awkward arrays and Python lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638e10e-8b65-40e7-a487-fdfa32f956e9",
   "metadata": {},
   "source": [
    "Let's jump into a quick example reading a Parquet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6245c24-48c6-4607-a9ba-4f80d29e74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_awkward as dak\n",
    "pq_dir = os.path.join(\"data\", \"parquet\")\n",
    "dataset = dak.from_parquet(pq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf75914-471b-4688-b9d5-0ed2ce3bad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa0243-d1bf-4d7f-a317-bea7090ce32a",
   "metadata": {},
   "source": [
    "By default the `dak.from_parquet` function will partition by file. Since the directory has four files, we will have four partitions in our Dask collection.\n",
    "\n",
    "Without computing anything, the `from_parquet` function has extracted some metadata, so we are able to get a peek at the structure of what the eventual awkward array would be if we did compute this collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91228e47-838a-42bb-a2e5-be9b47a6683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset._meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e35098-1790-4cf4-9543-d2bc5cc94908",
   "metadata": {},
   "source": [
    "You'll notice this dataset has two top level fields:\n",
    "- `type`\n",
    "- `scoring`\n",
    "\n",
    "Inside of the `scoring` field (or Parquet column) are three subfields (`scoring` is a `Record` in awkward array terminology):\n",
    "\n",
    "- `player`\n",
    "- `basket`\n",
    "- `distance`\n",
    "\n",
    "We can also see that for each element in the top level array, we have exactly one entry for the `type` field, and some variable (showing array raggedness) number of `scoring` entries.\n",
    "\n",
    "The data we have here is some made up data about basketball games/matches. Each game is labeled as either a \"friendly\" match or a \"league\" match. Each game has some number of total scores, each score being made by some player as some type of basket at some distance. The raggedness of the array comes from each match having a different total number of scores.\n",
    "\n",
    "Since this first section of the tutorial is meant to show the basics of the IO functions, we won't worry too much about the details of the dataset, but we will revisit the structure in the next section!\n",
    "\n",
    "Since this tutorial is using a small toy dataset we can easily compute it quickly to see a concrete awkward array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a519e29-2f87-47cd-8490-efa8a19eada5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "computed_dataset = dataset.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b148260-ae3a-4c7c-b666-bd0263d14ccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "computed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581deaea-e157-4e08-9cb7-49de574a009c",
   "metadata": {},
   "source": [
    "With parquet, we can restrict our data reading to only grab a specific set of columns from the files. In this toy dataset we're working with, if we only care about the specific players which did some scoring, we can specific that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab1ce4-d972-4c25-97dd-fe58fa9f77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dak.from_parquet(pq_dir, columns=[\"scoring.player\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca01b0-546c-4569-96b5-bed86a003648",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset._meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed602dd-b5cc-4535-937e-8d153272bd0d",
   "metadata": {},
   "source": [
    "Notice that when we peek at the metadata now, we see our array is going to contain less information, as expected! If we tied to access one of the fields we didn't request, we'd hit an `AttributeError` (before compute time!). Since we are able to track metadata at graph construction time, we can fail as early as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63742ac7-c18d-4801-9fda-6030a0c4e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.scoring.distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab1067-fa0f-46df-b055-27463795d271",
   "metadata": {},
   "source": [
    "Let's go back to the original dataset and save it to JSON after repartitioning the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d11eb1-f822-4fc9-ae0b-c10fb6c8ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dak.from_parquet(pq_dir)\n",
    "smaller_partition_dataset = dataset.repartition(15)\n",
    "dak.to_json(smaller_partition_dataset, os.path.join(\"data\", \"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17805aaa-26c4-4c2d-8664-bbe842d87c56",
   "metadata": {},
   "source": [
    "`dask-awkward`'s `to_*` functions have a bit of special treatmeant compared to other dask-awkward functions. They are the only parts of dask-awkward that are _eagerly_ computed. The `to_*` functions have a `compute=` argument that defaults to `True`. If you'd like to stage a data writing step without compute, you can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88a084-6d83-4eb7-a4a8-7befe58543d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_it = dak.to_json(smaller_partition_dataset, os.path.join(\"data\", \"json2\"), compute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532776bb-0789-45d0-9bd8-d108d5143f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b9879-c0d6-43a3-ad91-5f32209617ee",
   "metadata": {},
   "source": [
    "Notice that the `write_it` object is a dask-awkward `Scalar` collection that can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d00ee-2ec1-455e-b0e8-4c64f6e8d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_it.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8e243-9b83-4165-8073-9021e835ba09",
   "metadata": {},
   "source": [
    "Now we can reload our data with `dak.from_json`. Realistically, taking data stored in parquet to then save it as JSON to be read later is likely a bad idea! But we're just doing this to show example usage of the dask-awkward API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ea8ad-8ca6-444c-86cd-a4a4d9fc853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dak.from_json(os.path.join(\"data\", \"json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef65b6-793e-40df-b2fa-f8c74b2ee8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eddfcb-ab19-44dd-b8e5-f72b73716aad",
   "metadata": {},
   "source": [
    "## II. Column (buffer) optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb43949-40bd-4fdf-bd80-99c0a7b9376e",
   "metadata": {},
   "source": [
    "Dask workflows can be separated into two stages: first is task graph construction, and second is task graph execution. During task graph construction we are able to track metadata about our awkward array collections; with that metadata knowledge we are able, just before execution time, to know which parts of the Array are necessary to complete a computation. This is possible by running the task graph on a metadata only version of the arrays. When we run the metadata task graph, components of the data-less array are \"touched\" by the execution of the graph, and when that happens we know that's a part of the data on disk that needs to be read. \n",
    "\n",
    "Let's look at a quick example with Parquet. Recall the dataset from the previou section. We have these columns:\n",
    "\n",
    "- `type`\n",
    "- `scoring.player`\n",
    "- `scoring.basket`\n",
    "- `scoring.distance`\n",
    "\n",
    "If we want to calculate the average distance of each scored basket during each game, ignoreing all freethrows, we can calculate that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fcb593-63d5-4444-9d26-d0e23258f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dak.from_parquet(pq_dir)\n",
    "free_throws = dak.str.match_substring(dataset.scoring.basket, \"freethrow\")\n",
    "distances = dataset.scoring.distance[free_throws == False]\n",
    "result = dak.mean(distances, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a0003-f538-4eac-a4d9-15006e6fdc7f",
   "metadata": {},
   "source": [
    "The `result` will be the average distance of each non-free-throw shot. Notice we only used two of the four columns: `scoring.basket` and `scoring.distance`, If we wanted to be explicit about it, we could use the `columns=` argument in the `dak.from_parquet` call. But we can also just rely on dask-awkward to do this for us! The columns/buffer optimization will detect that the graph is only going to need those columns, rewriting the internal `ak.from_parquet` call at the node in the task graph that actually reads the data from disk. We can actually see this logic without running the compute with the `dak.necessary_columns` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b3ebe-1454-4dca-bee0-50a31f9c0df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dak.necessary_columns(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4e54c-33c1-42a4-9b0f-81bf6348b6d1",
   "metadata": {},
   "source": [
    "We see the name of the input layer, and the names of the columns that are going to be read by that input layer.\n",
    "\n",
    "This will also work with JSON. Awkward-Array's `from_json` has a feature that allows users to pass in a JSONSchema that instructs the reader which parts of the JSON dataset should be read. The reader still has to process all of the bytes in the text based file format but with a schema declared, the reader can intelligently skip over different keys in the JSON, saving memory and and time during array building.\n",
    "\n",
    "Here's the same computation but starting with a JSON dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13914bf9-1f45-4860-8dc7-ec8eeb746bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dak.from_json(os.path.join(\"data\", \"json\"))\n",
    "free_throws = dak.str.match_substring(dataset.scoring.basket, \"freethrow\")\n",
    "distances = dataset.scoring.distance[free_throws == False]\n",
    "result = dak.mean(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6e94b-ee5e-42d1-b789-6f80859b1d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dak.necessary_columns(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9380f6-c4d4-4a2b-bb99-5e15e1da9039",
   "metadata": {},
   "source": [
    "We see the exact same necessary columns.\n",
    "\n",
    "A final little detail. The way that we generate the JSON schema which is then passed to the reading node is with `dak.layout_to_jsonschema`. Once the column/buffer optimization has determined which are the fields will be necessary, we can select those fields from the awkward array form that we start with after the `dak.from_json` call. We then generate an awkward array layout from the sub-form generated by selecting a subset of the columns. Finally, we create a JSONSchema from that layout:\n",
    "\n",
    "In our small example case here, we know the columns are `scoring.basket` and `scoring.distance`. We can show this step manually here (starting with the first array collection created with the `dak.from_json call):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444fa35-03ee-4292-8730-490dacd145fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the subform based on the columns we need:\n",
    "subform = dataset.form.select_columns([\"scoring.basket\", \"scoring.distance\"])\n",
    "# create an awkward array layout:\n",
    "sublayout = subform.length_zero_array(highlevel=False)\n",
    "# and convert that to JSONSchema:\n",
    "necessary_schema = dak.layout_to_jsonschema(sublayout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a84b4-26ce-45c5-ad16-9c8967b60214",
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0952470-5557-4dca-919b-84970788cfdd",
   "metadata": {},
   "source": [
    "This feature can be turned off when running dask-awkward graphs with the config parameter \"awkward.optimization.enabled\". By default this setting is `True`. We can run the same compute with the feature turned off via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d75df4-c8a7-4abd-942c-f1e94c124ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "with dask.config.set({\"awkward.optimization.enabled\": False}):\n",
    "    result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b953ff-8edf-46a6-b069-c2aa6f802485",
   "metadata": {},
   "source": [
    "This could be useful for debugging. If the compute fails with the optimization enabled, but succeeds with the optimization disabled, then there is likely a bug in dask-awkward or awkward-array that should be raised!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ec861-c78a-4dbd-a306-388f0f88e9ec",
   "metadata": {},
   "source": [
    "Here's why we have each of the methods!\n",
    "\n",
    "- Starting with inheriting the `ColumnProjectionMixin`, inheriting from this mixin makes the class compatible with column optimization.\n",
    "- `__init__`: of course this is needed. It's going to take the starting form that the array should have, a tuple of exceptions that will be allowed to be raised at compute time that we can gracefully absorb, the columns to read, the awkward-array behavior that should be used, and additional kwargs that should be passed at each node's call of `ak.from_parquet\n",
    "- `return_report`: a class property that will tell `from_map` whether or not we will also return a report array\n",
    "- `use_optimization`: a class property that tells the columns optimization that we want this function class to be columns optimizable.\n",
    "- `report_success`: a static method that will be used to construct an report array when the read is successful at a partition\n",
    "- `report_failure`: the parter to `report_success`, if one of the allowed exceptions is raised at a partition at array creation time, this method will be called to construct an report array\n",
    "- `mock`: a method that \"mocks\" the array that would be created, returns a dataless typetracer array\n",
    "- `mock_empty`: a method that mocks the array but is not a typetracer array, it's an empty concrete awkward array. This is the method that is used at nodes that fail with an allowed exception.\n",
    "- `read_from_disk`: this is the method that will be called to... read data from disk! What actually matters more is the next method:\n",
    "- `__call__`: we finally get to the \"function\" part of this class: This method will be called at each partition. You'll notice that we call `read_from_disk` here, but we wrap it in a `try`, `except` block if we want to return the read-report that allows for graceful fails\n",
    "- `project_columns`: this method is necessary for rewriting the class instructing it to read a new set of columns. This method is part of the optimization interface\n",
    "\n",
    "Finally, we write a function that is going to use this function class and call `from_map\n",
    "\n",
    "Let's use it to read our parquet dataset and look at both the resulting array and the post-compute report. Notice that the report itself is a lazily evaluated dask-awkward Array collection that should be computed simultaneously with the collection-of-interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9df59-27d6-4b8c-b234-ebbd2e231393",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, report = my_read_parquet(pq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428be39-9c57-4f2a-8399-3a4023379782",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, computed_report = dask.compute(dataset, report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db71ab-d7f6-4e0d-80c6-56453405b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ac8f6-f42d-4bc7-b5da-97dc7678d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_report.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89375b-96ca-4f51-b084-c7c48943f7fb",
   "metadata": {},
   "source": [
    "We can see in the report that the file with a \"0\" in the name indeed failed!\n",
    "\n",
    "You'll see that we added the columns that are read to the report as well, so if we perform a compute that will only need a subset of the columns, we can get confirmation from our report array. We get the column optimization by inheriting from the column optimization mixin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff497b-1e86-4760-9c03-3f5e58cbba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dak.necessary_columns(dataset.scoring.player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316e4d2-2a22-4607-9c66-ca390d7cffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, computed_report= dask.compute(dataset.scoring.player, report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a849a14-a95d-46ac-a488-586b0bb3f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_report.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
