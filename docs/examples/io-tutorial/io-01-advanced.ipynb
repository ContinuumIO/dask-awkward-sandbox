{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8096427e-e5c5-4b5c-9d74-b172d786caa1",
   "metadata": {},
   "source": [
    "# Advanced Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa55e3-592a-419f-a938-78b11889adc3",
   "metadata": {},
   "source": [
    "*Before reading this notebook we recommend reading [the basic notebook first!](io-00-basic.ipynb)*\n",
    "\n",
    "_last updated 2024-01-26_\n",
    "\n",
    "All of the high level file format readers in `dask-awkward` are based on a lower level API: the `from_map` function. This function provides an interface that allows any user defined function to be used as a source of awkward arrays at the nodes in a Dask graph.\n",
    "\n",
    "A very simple usage of the `from_map` API would be to re-create `from_parquet`:\n",
    "\n",
    "```python\n",
    "dak.from_map(\n",
    "    ak.from_parquet,\n",
    "    [\"/path/to/some/file1.parquet\", \"/path/to/some/file2.parquet\"],\n",
    "    label=\"my-from-parquet\",\n",
    ")\n",
    "```\n",
    "\n",
    "This will create a `dask-awkward` collection that calls `ak.from_parquet` on those two files, which as stated above, is a simple recreation of `dak.from_parquet` (obviously less flexible/powerful than `from_parquet`! but one should get the idea)\n",
    "\n",
    "The power of `from_map` materializes when one would like to take advantage of column optimization or gracefully fail, returning an empty array instead of a program crashing, at some nodes where read issues surface. We can begin to demonstrate these features by defining a function class to be passed in as the first argument to `from_map`.\n",
    "\n",
    "Our example will be a special Parquet reader that rejects any file that contains a \"0\" in the filename. For some reason we've found that data to be corrupt, but we want to be able still process the whole directory and not manually skip those files\n",
    "\n",
    "We'll write out the class implementation and then explain each of the methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ade1b2-8988-4147-a53b-f6837aef4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "from dask_awkward.lib.io.columnar import ColumnProjectionMixin\n",
    "\n",
    "class Ignore0ParquetReader(ColumnProjectionMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        form: Form,\n",
    "        report: bool = False,\n",
    "        allowed_exceptions: tuple[type[BaseException], ...] = (OSError,),\n",
    "        columns: list[str] | None = None,\n",
    "        behavior: dict | None = None,\n",
    "        **kwargs: Any\n",
    "    ):\n",
    "        self.form = form\n",
    "        self.report = report\n",
    "        self.allowed_exceptions = allowed_exceptions\n",
    "        self.columns = columns\n",
    "        self.behavior = behavior\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    @property\n",
    "    def return_report(self) -> bool:\n",
    "        return self.report\n",
    "\n",
    "    @property\n",
    "    def use_optimization(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def report_success(source, columns) -> ak.Array:\n",
    "        return ak.Array([{\"source\": source, \"exception\": None, \"columns\": columns}])\n",
    "\n",
    "    @staticmethod\n",
    "    def report_failure(source, exception) -> ak.Array:\n",
    "        return ak.Array([{\"source\": source, \"exception\": repr(exception), \"columns\": None}])\n",
    "\n",
    "    def mock(self) -> ak.Array:\n",
    "        return ak.typetracer.typetracer_from_form(self.form, highlevel=True)\n",
    "\n",
    "    def mock_empty(self, backend=\"cpu\") -> ak.Array:\n",
    "        return ak.to_backend(self.form.length_one_array(highlevel=False), backend=backend, highlevel=True)\n",
    "\n",
    "    def read_from_disk(self, source: Any) -> ak.Array:\n",
    "        if \"0\" in source:\n",
    "            raise OSError(\"cannot read files that contain '0' in the name\")\n",
    "        return ak.from_parquet(source, columns=self.columns, **self.kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        source = args[0]\n",
    "        if self.return_report:\n",
    "            try:\n",
    "                array = self.read_from_disk(source)\n",
    "                return array, self.report_success(source, self.columns)\n",
    "            except self.allowed_exceptions as err:\n",
    "                array = self.mock_empty()\n",
    "                return array, self.report_failure(source, err)\n",
    "        else:\n",
    "            return self.read_from_disk(source)        \n",
    "\n",
    "    def project_columns(self, columns):\n",
    "        return Ignore0ParquetReader(\n",
    "            form=self.form.select_columns(columns),\n",
    "            report=self.return_report,\n",
    "            allowed_exceptions=self.allowed_exceptions,\n",
    "            columns=columns,\n",
    "            **self.kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "def my_read_parquet(path, columns=None, allowed_exceptions=(OSError,)):\n",
    "    pq_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\"parquet\")]\n",
    "    meta_from_pq = ak.metadata_from_parquet(pq_files)\n",
    "    form = meta_from_pq[\"form\"]\n",
    "    fn = Ignore0ParquetReader(form, report=True, allowed_exceptions=allowed_exceptions)\n",
    "    return dak.from_map(fn, pq_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d176ec9-ab45-425c-a680-2f1adba0b32c",
   "metadata": {},
   "source": [
    "Here's why we have each of the methods!\n",
    "\n",
    "- Starting with inheriting the `ColumnProjectionMixin`, inheriting from this mixin makes the class compatible with column optimization.\n",
    "- `__init__`: of course this is needed. It's going to take the starting form that the array should have, a tuple of exceptions that will be allowed to be raised at compute time that we can gracefully absorb, the columns to read, the awkward-array behavior that should be used, and additional kwargs that should be passed at each node's call of `ak.from_parquet\n",
    "- `return_report`: a class property that will tell `from_map` whether or not we will also return a report array\n",
    "- `use_optimization`: a class property that tells the columns optimization that we want this function class to be columns optimizable.\n",
    "- `report_success`: a static method that will be used to construct an report array when the read is successful at a partition\n",
    "- `report_failure`: the parter to `report_success`, if one of the allowed exceptions is raised at a partition at array creation time, this method will be called to construct an report array\n",
    "- `mock`: a method that \"mocks\" the array that would be created, returns a dataless typetracer array\n",
    "- `mock_empty`: a method that mocks the array but is not a typetracer array, it's an empty concrete awkward array. This is the method that is used at nodes that fail with an allowed exception.\n",
    "- `read_from_disk`: this is the method that will be called to... read data from disk! What actually matters more is the next method:\n",
    "- `__call__`: we finally get to the \"function\" part of this class: This method will be called at each partition. You'll notice that we call `read_from_disk` here, but we wrap it in a `try`, `except` block if we want to return the read-report that allows for graceful fails\n",
    "- `project_columns`: this method is necessary for rewriting the class instructing it to read a new set of columns. This method is part of the optimization interface\n",
    "\n",
    "Finally, we write a function that is going to use this function class and call `from_map\n",
    "\n",
    "Let's use it to read our parquet dataset and look at both the resulting array and the post-compute report. Notice that the report itself is a lazily evaluated dask-awkward Array collection that should be computed simultaneously with the collection-of-interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00b299a-dcc3-4106-a314-86475345f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_dir = os.path.join(\"data\", \"parquet\")\n",
    "dataset, report = my_read_parquet(pq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64fbcdc5-f1e2-44bf-81a0-ecaf375662be",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, computed_report = dask.compute(dataset, report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d32383a-a09d-44bc-9a2c-4b1f28b6eb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[{type: &#x27;&#x27;, scoring: []},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " ...,\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]}]\n",
       "--------------------------------------------------\n",
       "type: 151 * {\n",
       "    type: string,\n",
       "    scoring: var * {\n",
       "        player: string,\n",
       "        basket: string,\n",
       "        distance: float64\n",
       "    }\n",
       "}</pre>"
      ],
      "text/plain": [
       "<Array [{type: '', scoring: []}, ..., {...}] type='151 * {type: string, sco...'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1202361-6b24-4400-bf7e-632d744e13b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'data/parquet/part0.parquet',\n",
       "  'exception': 'OSError(\"cannot read files that contain \\'0\\' in the name\")',\n",
       "  'columns': None},\n",
       " {'source': 'data/parquet/part2.parquet',\n",
       "  'exception': None,\n",
       "  'columns': ['type', 'scoring.distance', 'scoring.basket', 'scoring.player']},\n",
       " {'source': 'data/parquet/part3.parquet',\n",
       "  'exception': None,\n",
       "  'columns': ['type', 'scoring.distance', 'scoring.basket', 'scoring.player']},\n",
       " {'source': 'data/parquet/part1.parquet',\n",
       "  'exception': None,\n",
       "  'columns': ['type', 'scoring.distance', 'scoring.basket', 'scoring.player']}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_report.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c1aa5-72cd-4baa-abbe-a3791d3f3d4a",
   "metadata": {},
   "source": [
    "We can see in the report that the file with a \"0\" in the name indeed failed!\n",
    "\n",
    "You'll see that we added the columns that are read to the report as well, so if we perform a compute that will only need a subset of the columns, we can get confirmation from our report array. We get the column optimization by inheriting from the column optimization mixin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68910a0-b62e-483e-9098-25424850a05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<__main__.Ignore0ParquetReader object at 0x7fbc1c5-98de39e045724a64b44ebd0cc521dc4e': frozenset({'scoring.player'})}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dak.necessary_columns(dataset.scoring.player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb43bb7a-047b-4cda-b124-40c671448b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, computed_report= dask.compute(dataset.scoring.player, report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17851b18-4ae6-4bd8-a562-3cf816a39a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'data/parquet/part0.parquet',\n",
       "  'exception': 'OSError(\"cannot read files that contain \\'0\\' in the name\")',\n",
       "  'columns': None},\n",
       " {'source': 'data/parquet/part2.parquet',\n",
       "  'exception': None,\n",
       "  'columns': ['scoring.player']},\n",
       " {'source': 'data/parquet/part3.parquet',\n",
       "  'exception': None,\n",
       "  'columns': ['scoring.player']},\n",
       " {'source': 'data/parquet/part1.parquet',\n",
       "  'exception': None,\n",
       "  'columns': ['scoring.player']}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_report.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
